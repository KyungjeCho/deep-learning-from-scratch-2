{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4597ce81",
   "metadata": {},
   "source": [
    "<h1> ch8 어텐션 </h1>\n",
    "\n",
    "<p>\n",
    "    RNN을 사용한 문장 생성을 했다. RNN 2개로 seq2seq를 만들어 덧셈 문제도 풀었다. 이제는 어텐션을 학습한다.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1124a3",
   "metadata": {},
   "source": [
    "<h2> 8.1 어텐션의 구조 </h2>\n",
    "\n",
    "<p>\n",
    "    어텐션 메커니즘로 seq2seq 처럼 필요한 정보에만 '주목'할 수 있다. 게다가 seq2seq가 가진 문제를 해결할 수 있다. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed90db7",
   "metadata": {},
   "source": [
    "<h3> 8.1.1 seq2seq의 문제점 </h3>\n",
    "\n",
    "<p>\n",
    "    seq2seq는 Encoder가 시계열 데이터를 인코딩한다. 인코딩된 정보를 Decoder로 전달한다. Encoder의 출력은 '고정 길이의 벡터'이다. 고정 길이 벡터라 함은 입력 문장의 길이에 관계없이 항상 같은 길이의 벡터로 변환한다는 것이다. 긴 문장을 억지로 욱여넣듯이 고정 길이의 벡터로 밀어 넣는다. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b078de",
   "metadata": {},
   "source": [
    "<h3> 8.1.2 Encoder 개선 </h3>\n",
    "\n",
    "<p>\n",
    "    마지막 은닉 상태만을 Decoder에 전달했다. 이것을 Encoder 출력의 길이는 입력 문장의 길이에 따라 매번 바꿔주는 것이 좋다. 예를 들어, LSTM 계층의 은닉 상태 벡터를 모두 이용하는 것이다. 모든 시각의 은닉 상태 벡터를 모두 이용한다면 입력한 단어 수의 벡터를 얻을 수 있다. LSTM 계층의 은닉 상태의 '내용'은 시각별 LSTM 계층에 직전에 입력된 단어에 대한 정보가 많이 포함되어 있다. Encoder가 출력하는 hs 행렬은 각 단어에 해당하는 벡터들의 집합이다.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24a98e",
   "metadata": {},
   "source": [
    "<h3> 8.1.3 Decoder 개선 1</h3>\n",
    "\n",
    "<p>\n",
    "    Encoder는 각 단어에 대응하는 LSTM 계층의 은닉 상태 벡터를 hs에 모아 출력한다. 이전 seq2seq에서는 Encoder의 마지막 은닉상태 벡터만을 Decoder에 넘겼다. Decoder의 LSTM 계층의 '첫' 은닉 상태로 설정한다. 이 뜻은 현재 hs 벡터 집합에서 마지막 은닉 상태만 빼내서 Decoder에 전달한 것이다. \n",
    "</p>\n",
    "<p>\n",
    "    사람이 문장을 번역할 때는 '나'='I' 나 '고양이'='cat'이라는 지식을 활용한다. 어떤 단어에 주목하여 그 단어의 변환을 수시로 한다. 입력과 출력의 여러 단어 중 어떤 단어끼리 서로 관련되어 있는지를 학습 시 켜야한다. \n",
    "</p>\n",
    "<p>\n",
    "    '도착어 단어'와 대응 관계에 있는 '출발어 단어'의 정보를 골라내야 한다. 필요한 정보에만 주목하여 정보로부터 시계열 변환을 수행하는 것이 목표이다. 이 구조가 어텐션이다. Decoder에 LSTM 다음 계층에 '어떤 계산'을 수행하는 계층을 추가해야 한다. '어떤 계산'의 입력은 두가지, hs와 LSTM 계층의 은닉 상태이다. 여기서 필요한 정보만 골라 위쪽 Affine 계층으로 출력한다. Decoder에 입력한 단어와 대응 관계인 단어 벡터를 hs를 통해 골라내야 한다. 이러한 '선택' 작업을 통해 '어떤 계산'을 하는 것이지만, 이러한 선택하는 작업은 미분이 불가능하다. '선택한다'는 작업을 미분 가능한 연산으로 대체해야 한다. '하나를 선택'하는 것이 아니라, '모든 것을 선택'하여 각 단어의 중요도를 나타내는 '가중치'를 별도로 계산한다. 각 단어의 중요도를 나타내는 '가중치' (a) 를 이용한다. a는 확률 분포처럼 각 원소가 0.0 ~ 1.0 사이의 스칼라이며, 원소의 총합은 1이다. 가중치 a와 hs로부터 가중합을 구하여, 벡터를 얻는다. 결과 벡터를 '맥락 벡터'라고 부르고 기호는 c이다. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0ac44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "(5, 4)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1)\n",
    "print(ar.shape)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=0)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83a5603a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "# 미니배치 처리용 가중합\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N, T)\n",
    "ar = a.reshape(N, T, 1).repeat(H,  axis=2)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis=1)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd87010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        t = hs * ar\n",
    "        \n",
    "        c = np.sum(t, axis=1)\n",
    "        \n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "        \n",
    "        return dhs, da        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ed7867",
   "metadata": {},
   "source": [
    "<h3> 8.1.4 Decoder 개선 2 </h3>\n",
    "\n",
    "<p>\n",
    "    각 단어의 중요도를 나타내는 a가 있으면 '맥락 벡터'를 얻을 수 있다. a 또한 자동으로 학습할 수 있어야 한다. Decoder 은닉 상태 벡터와 hs의 각 단어 벡터와 얼마나 '비슷한가'를 수치로 나타낸다. 수치를 나타내는 가장 단순한 방법인 벡터의 '내적'을 이용한다. \n",
    "    $$\n",
    "        \\mathbf{a\\cdot b} = a_1b_1 + a_2b_2 + \\cdots + a_nb_n\n",
    "    $$\n",
    "    벡터의 내적은 두 벡터가 얼마나 같은 방향을 향하고 있는가 이다. h와 hs의 각 단어 벡터와의 유사도를 구한다. s는 그 결과고, s는 정규화하기 전 값이여서, 소프트맥스 함수로 정규화해서 적용한다.\n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6eada9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 5)\n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "h = np.random.randn(N, H)\n",
    "hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "\n",
    "t = hs * hr\n",
    "print(t.shape)\n",
    "\n",
    "s = np.sum(t, axis=2)\n",
    "print(s.shape)\n",
    "\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4659e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *\n",
    "from common.layers import Softmax\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "        \n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "        \n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "        \n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c177e5",
   "metadata": {},
   "source": [
    "<h3> 8.1.5 Decoder 개선 3</h3>\n",
    "\n",
    "<p>\n",
    "    AttentionWeight 계층과 WeightSum 계층을 각각 구현하였고, 두 계층을 하나로 결합한다. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda91260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0ad998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "        \n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_des[:, t, :])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "            \n",
    "        return  out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:, t, :] = dh\n",
    "            \n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e7e86",
   "metadata": {},
   "source": [
    "<h2> 8.2 어텐션을 갖춘 seq2seq 구현 </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a2d4f",
   "metadata": {},
   "source": [
    "<h3> 8.2.1 Encoder 구현 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e95603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from ch07.seq2seq import Encoder, Seq2seq\n",
    "from ch08.attention_layer import TimeAttention\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62835887",
   "metadata": {},
   "source": [
    "<h3> 8.2.2 Decoder 구현 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "317c8f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2 * H, V) / np.sqrt(2 * H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def backward(self, dscore):\n",
    "        # 깃허브 소스 참고\n",
    "        pass\n",
    "    \n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        # 깃허브 소스 참고\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda2a77",
   "metadata": {},
   "source": [
    "<h3> 8.2.3 seq2seq 구현 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f16dbe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ch07.seq2seq import Encoder, Seq2seq\n",
    "\n",
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49e3354",
   "metadata": {},
   "source": [
    "<h2> 8.3 어텐션 평가 </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7542c396",
   "metadata": {},
   "source": [
    "<h3> 8.3.1 날짜 형식 변환 문제 </h3>\n",
    "\n",
    "<p>\n",
    "    다양한 날짜 형식을 표준 형식으로 변환하는 것이 목표이다. 날짜 데이터는 다양한 변형이 존재하여 변환 규칙이 나름 복잡하다. 입력과 출력 사이에 알기 쉬운 대응 관계도 있다.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c95fd",
   "metadata": {},
   "source": [
    "<h3> 8.3.2 어텐션을 갖춘 seq2seq의 학습 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c7ae44b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 5[s] | 손실 3.09\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 10[s] | 손실 1.90\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 15[s] | 손실 1.72\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 21[s] | 손실 1.46\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 26[s] | 손실 1.19\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 31[s] | 손실 1.14\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 37[s] | 손실 1.09\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 42[s] | 손실 1.06\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 47[s] | 손실 1.04\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 53[s] | 손실 1.03\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 58[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 63[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 68[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 74[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 79[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 84[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 89[s] | 손실 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1978-08-11\n",
      "---\n",
      "val acc 0.000%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 21 / 351 | 시간 5[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 41 / 351 | 시간 10[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 61 / 351 | 시간 15[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 81 / 351 | 시간 20[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 101 / 351 | 시간 25[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 121 / 351 | 시간 30[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 141 / 351 | 시간 35[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 161 / 351 | 시간 41[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 181 / 351 | 시간 46[s] | 손실 0.97\n",
      "| 에폭 2 |  반복 201 / 351 | 시간 51[s] | 손실 0.95\n",
      "| 에폭 2 |  반복 221 / 351 | 시간 56[s] | 손실 0.94\n",
      "| 에폭 2 |  반복 241 / 351 | 시간 61[s] | 손실 0.90\n",
      "| 에폭 2 |  반복 261 / 351 | 시간 66[s] | 손실 0.83\n",
      "| 에폭 2 |  반복 281 / 351 | 시간 71[s] | 손실 0.74\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 76[s] | 손실 0.66\n",
      "| 에폭 2 |  반복 321 / 351 | 시간 81[s] | 손실 0.58\n",
      "| 에폭 2 |  반복 341 / 351 | 시간 87[s] | 손실 0.46\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2016-11-08\n",
      "---\n",
      "val acc 51.660%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 3 |  반복 21 / 351 | 시간 5[s] | 손실 0.30\n",
      "| 에폭 3 |  반복 41 / 351 | 시간 10[s] | 손실 0.21\n",
      "| 에폭 3 |  반복 61 / 351 | 시간 16[s] | 손실 0.14\n",
      "| 에폭 3 |  반복 81 / 351 | 시간 21[s] | 손실 0.09\n",
      "| 에폭 3 |  반복 101 / 351 | 시간 26[s] | 손실 0.07\n",
      "| 에폭 3 |  반복 121 / 351 | 시간 31[s] | 손실 0.05\n",
      "| 에폭 3 |  반복 141 / 351 | 시간 37[s] | 손실 0.04\n",
      "| 에폭 3 |  반복 161 / 351 | 시간 42[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 181 / 351 | 시간 47[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 201 / 351 | 시간 52[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 221 / 351 | 시간 58[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 241 / 351 | 시간 63[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 261 / 351 | 시간 68[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 281 / 351 | 시간 73[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 78[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 321 / 351 | 시간 84[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 341 / 351 | 시간 89[s] | 손실 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.900%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 21 / 351 | 시간 5[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 41 / 351 | 시간 10[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 61 / 351 | 시간 15[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 81 / 351 | 시간 20[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 101 / 351 | 시간 26[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 121 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 141 / 351 | 시간 36[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 161 / 351 | 시간 42[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 181 / 351 | 시간 47[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 201 / 351 | 시간 52[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 221 / 351 | 시간 57[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 241 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 261 / 351 | 시간 68[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 281 / 351 | 시간 73[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 78[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 321 / 351 | 시간 83[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 341 / 351 | 시간 88[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.900%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 21 / 351 | 시간 5[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 41 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 61 / 351 | 시간 15[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 81 / 351 | 시간 21[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 101 / 351 | 시간 26[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 121 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 141 / 351 | 시간 36[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 161 / 351 | 시간 41[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 181 / 351 | 시간 46[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 201 / 351 | 시간 52[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 221 / 351 | 시간 57[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 241 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 261 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 281 / 351 | 시간 73[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 78[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 321 / 351 | 시간 83[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 341 / 351 | 시간 88[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.920%\n",
      "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 21 / 351 | 시간 5[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 41 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 61 / 351 | 시간 15[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 81 / 351 | 시간 20[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 101 / 351 | 시간 26[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 121 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 141 / 351 | 시간 36[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 161 / 351 | 시간 41[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 181 / 351 | 시간 46[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 201 / 351 | 시간 51[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 221 / 351 | 시간 56[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 241 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 261 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 281 / 351 | 시간 72[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 301 / 351 | 시간 77[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 321 / 351 | 시간 82[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 341 / 351 | 시간 87[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 99.920%\n",
      "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 21 / 351 | 시간 5[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 41 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 61 / 351 | 시간 16[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 81 / 351 | 시간 21[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 101 / 351 | 시간 26[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 121 / 351 | 시간 32[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 141 / 351 | 시간 37[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 161 / 351 | 시간 42[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 181 / 351 | 시간 48[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 201 / 351 | 시간 53[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 221 / 351 | 시간 58[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 241 / 351 | 시간 64[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 261 / 351 | 시간 69[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 281 / 351 | 시간 74[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 301 / 351 | 시간 80[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 321 / 351 | 시간 85[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 341 / 351 | 시간 90[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.960%\n",
      "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 21 / 351 | 시간 5[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 41 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 61 / 351 | 시간 15[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 81 / 351 | 시간 20[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 101 / 351 | 시간 26[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 121 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 141 / 351 | 시간 36[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 161 / 351 | 시간 41[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 181 / 351 | 시간 47[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 201 / 351 | 시간 52[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 221 / 351 | 시간 57[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 241 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 261 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 281 / 351 | 시간 73[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 301 / 351 | 시간 78[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 321 / 351 | 시간 83[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 341 / 351 | 시간 88[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.960%\n",
      "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 21 / 351 | 시간 5[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 41 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 61 / 351 | 시간 15[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 81 / 351 | 시간 20[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 101 / 351 | 시간 26[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 121 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 141 / 351 | 시간 36[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 161 / 351 | 시간 41[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 181 / 351 | 시간 46[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 201 / 351 | 시간 52[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 221 / 351 | 시간 57[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 241 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 261 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 281 / 351 | 시간 72[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 301 / 351 | 시간 77[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 321 / 351 | 시간 83[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 341 / 351 | 시간 88[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.960%\n",
      "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 21 / 351 | 시간 5[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 41 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 61 / 351 | 시간 16[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 81 / 351 | 시간 21[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 101 / 351 | 시간 26[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 121 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 141 / 351 | 시간 36[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 161 / 351 | 시간 42[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 181 / 351 | 시간 47[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 201 / 351 | 시간 52[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 221 / 351 | 시간 57[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 241 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 261 / 351 | 시간 68[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 281 / 351 | 시간 73[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 301 / 351 | 시간 78[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 321 / 351 | 시간 83[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 341 / 351 | 시간 88[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.960%\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../ch07')\n",
    "import numpy as np\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "from ch07.seq2seq import Seq2seq\n",
    "from ch07.peeky_seq2seq import PeekySeq2seq\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "               batch_size=batch_size, max_grad=max_grad)\n",
    "    \n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                   id_to_char, verbose, is_reverse=True)\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc * 100))\n",
    "    \n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cfa6f1",
   "metadata": {},
   "source": [
    "<h2> 8.4 어텐션에 관한 남은 이야기 </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bfbcfd",
   "metadata": {},
   "source": [
    "<h3> 8.4.1 양반향 RNN </h3>\n",
    "\n",
    "<p>\n",
    "    LSTM의 각 시각의 은닉 상태 벡터는 hs로 모아진다. 단어의 '주변' 정보를 균형 있게 담고 싶을 것이다. LSTM을 양반향으로 처리하는 방법을 생각할 수 있다. 양방향으로 처리함으로써, 각 단어에 대응하는 은닉 상태 벡터에는 좌와 우 양쪽 방향으로부터 정보를 집약할 수 있다. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c274ed30",
   "metadata": {},
   "source": [
    "<h3> 8.4.2 Attention 계층 사용 방법 </h3>\n",
    "\n",
    "<p>\n",
    "    Attention 계층을 LSTM 계층과 Affine 계층 사이에 삽입했다. Attention 계층의 출력이 다음 시각의 LSTM 계층에 입력 되게 연결한다. 이렇게 하면 LSTM 계층이 맥락 벡터의 정보를 이용할 수 있다. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd62d8",
   "metadata": {},
   "source": [
    "<h3> 8.4.3 seq2seq 심층화와 skip 연결 </h3>\n",
    "\n",
    "<p>\n",
    "    현실 어플리케이션들은 풀어야 할 문제가 휠씬 복잡하다. 어텐션을 갖춘 seq2seq에도 더 높은 표현력이 요구될 것이다. RNN 계층을 깊게 쌓는 방법이다. 층을 깊게 할 때 사용되는 중요한 기법 중 skip 연결 이라는 것이 있다. skip 연결의 접속부에서는 2개의 출력이 '더해'진다. skip 연결의 기울기가 아무런 영향을 받지 않고 모든 계층으로 흐른다. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ec0df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd0934bad200ce2233862c51537af2a279b3e2fd7629fe4796e40f0bd88eed44efe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
