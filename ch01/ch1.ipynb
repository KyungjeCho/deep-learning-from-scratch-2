{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1>신경망 복습</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> 밑바닥부터 시작하는 딥러닝의 속편입니다. 이번 장에서 신경망을 복습합니다.</h3>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> 1. 수학과 파이썬 복습</h2>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "먼저 수학을 복습합니다. 신경망 계산을 위한 '벡터'와 '행렬'에 대해 복습합니다. <hr />"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>1-1 벡터와 행렬 </h3>\n",
    "<p> 신경망은 벡터와 행렬이 도처에 등장한다. </p>\n",
    "<p>\n",
    "    '벡터'는 <mark> 크기와 방향을 가진 양</mark> 이다. 숫자가 일렬로 늘어선 집합으로 표현하고 또 그렇게 쓴다. 파이썬은 벡터를 1차원 배열로 취급한다. 즉, 파이썬에서는 '벡터'는 숫자로 집합된 1차원 배열이라고 생각하면 된다. \n",
    "</p>\n",
    "<p>\n",
    "    '행렬'은 숫자가 <mark> 2차원 형태로 늘어선 것 </mark> 으로 벡터 두개를 붙여 놓은 것 같이 보인다. $ N \\times M $ 으로 표현한다. 또 행렬에서는 가로줄을 <mark>행</mark>, 세로줄을 <mark>열</mark>라고 한다.\n",
    "</p>\n",
    "<p>\n",
    "    $$ \\begin{pmatrix}1&2\\\\3&4\\\\5&6\\end{pmatrix} $$\n",
    "</p>\n",
    "<p>\n",
    "    벡터는 표현하는 방법이 두 가지입니다. 하나는 숫자를 <mark>세로로 나열하는 방법(열백터)</mark> 과 <mark> 다른 하나는 가로로 나열하는 방법(행백터)</mark> 입니다. 수학과 딥러닝 등 많은 분야에서 '열백터' 방식을 선호한다. 또한 수식에서 벡터나 행렬은 $\\textbf{x}$와 $\\textbf{W}$ 처럼 굵게 표기하여 단일 원소로 이뤄진 스칼라 값과 구별했습니다.\n",
    "</p>\n",
    "    "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "print(x.__class__) # x 의 클래스\n",
    "\n",
    "print(x.shape) # x의 벡터 차원 형상, x는 3차원 벡터\n",
    "# 결과 : (3, )\n",
    "\n",
    "print(x.ndim) # x의 숫자 집합 N 차원 수, x는 1차원 \n",
    "# 결과 : 1\n",
    "\n",
    "W = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(W.shape) # W의 행렬 차원 형상, W는 2 X 3 차원\n",
    "# 결과 : (2, 3)\n",
    "\n",
    "print(W.ndim) # W의 수자 집합 N 차원 수, W는 2차원\n",
    "# 결과 : 2"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3,)\n",
      "1\n",
      "(2, 3)\n",
      "2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    벡터와 행렬은 np.array() 메서드로 생성 가능하다. 이 메서드는 넘파이의 다차원 배열 클래스인 np.ndarray 클래스를 생성한다. np.ndarray 클래스에는 다양한 편의 메서드와 인스턴스 변수가 준비되어 있다. 그 중 shape와 ndim을 이용했다. shape는 <mark>다차원 배열의 형상</mark>을, ndim은 <mark>차원 수</mark>를 담고 있다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> 1-2 행렬의 원소별 연산</h3>\n",
    "<p> 이번에는 벡터와 행렬을 사용해 간단한 계산을 한다</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "W = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "X = np.array([[0, 1, 2], [3, 4, 5]]) # 두 개의 같은 형상(shape) 행렬을 생성했다.\n",
    "\n",
    "print(W + X) # 행렬 덧셈 계산\n",
    "\n",
    "print(W * X)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 1  3  5]\n",
      " [ 7  9 11]]\n",
      "[[ 0  2  6]\n",
      " [12 20 30]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    다차원 넘파이 배열의 사칙연산 중 더하기($+$)와 곱하기($\\times$)를 했다. 피연산자인 다차원 배열들 서로 같은 행과 열에 있는 원소끼리 연산이 이루어 진다. \n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> 1-3 브로드 캐스트</h3>\n",
    "<p> 넘파이에서 다차원 배열과 형상(shape)이 다른 배열끼리도 연산이 가능하다. 예를 들면, $ 2 \\times 2 $ 행렬과 스칼라랑 연산할 수 있다.</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "A = np.array([[1, 2], [3, 4]]) # 2 * 2 행렬 \n",
    "print(A * 10) # 2 * 2 행렬과 스칼라(10)이랑 곱셈 연산"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[10 20]\n",
      " [30 40]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    $ 2 \\times 2 $ 행렬 A에 10인 스칼라를 곱했다. 이러면 스칼라 값 10이 $ 2 \\times 2 $ 행렬로 확장, 늘려진 행렬로 만들어진 후 원소별 연산을 수행한다. 이 기능을 <mark>브로드캐스트(broadcast)</mark>라고 한다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "A = np.array([[1, 2], [3, 4]]) \n",
    "b = np.array([[10, 20]]) \n",
    "# 2 * 2 행렬과 벡터를 선언했다.\n",
    "\n",
    "print(A * b) # 행렬과 백터를 원소별 연산을 시켰다."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[10 40]\n",
      " [30 80]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    여기서는 b가 행렬 A에 맞쳐서 $ 2 \\times 2 $ 크기로 확장되어\n",
    "    $$\n",
    "        b =  \\begin{pmatrix}10&20\\\\10&20\\end{pmatrix}\n",
    "    $$\n",
    "    로 되었다. 물론 브로드캐스트가 효과적으로 작동하려면 몇가지 규칙을 충족해야 된다.\n",
    "\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>1-4 벡터의 내적과 행렬의 곱 </h3>\n",
    "<p> \n",
    "    벡터의 내적과 행렬의 곱셈을 한다. 우선 백터의 내적부터 시작한다. 백터의 내적의 수식은 \n",
    "    $$\n",
    "        \\textbf{x} \\cdot \\textbf{y} = x_1 y_1 + x_2 y_2 + \\cdots + x_n y_n\n",
    "    $$\n",
    "    이다. 벡터의 내적은 두 벡터에서 대응하는 원소들의 곱을 모두 더한 것이다. 이러면 결과는 스칼라가 된다. 백터의 내적은 직관적으로 <mark>두 벡터가 얼마나 같은 방향을 향하고 있는지</mark>  보여준다. 완전히 같은 방향이면 내적의 결과는 1이고, 반대 방향을 향하면 두 벡터의 내적은 -1이다.\n",
    "</p>\n",
    "<p>\n",
    "    행렬의 곱도 한다. 행렬의 곱은 '왼쪽 행렬의 행벡터(가로방향)'와 '오른쪽 행렬의 열벡터(세로방향)'의 내적으로 계산한다. 그리고 계산 결과는 새로운 행렬의 대응하는 원소에 저장한다. $\\textbf{A}$의 1행과 $\\textbf{B}$의 1열의 계산 결과는 1행1열 위치의 원소로 된다. $ 2 \\times 3 $ 행렬과 $ 3 \\times 2 $ 행렬을 곱하면 결과는 $ 2 \\times 2 $ 행렬이 된다. 그리고 왼쪽 행렬의 행벡터 차원과 오른쪽 행렬의 열벡터 차원의 수가 같아야 한다.\n",
    "</p>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# 벡터의 내적\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "print(np.dot(a, b)) # 벡터의 내적은 np.dot() 메서드를 이용한다. 두 벡터의 형상(shape)은 같다.\n",
    "\n",
    "# 행렬의 곱\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "print(np.matmul(A, B)) # 행렬의 곱은 np.matmul() 메서드를 이용한다."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32\n",
      "[[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    사실 벡터의 내적과 행렬의 곱 모두 np.dot() 메서드를 사용할 수 있다. np.dot(x, y)의 인수가 모두 1차원이면 벡터의 내적을 계산하고, 2차원 배열이면 행렬의 곱을 계산한다. 다만, 의도를 확실하게 해주어야 한다. 넘파이 경험을 쌓고 싶으면 '100 numpy exercise' 사이트를 추천한다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> 1-5 행렬 형상 확인 </h3>\n",
    "<p> \n",
    "    행렬이나 벡터를 사용할 때는 각각의 형상(shape)에 주의해야 한다. '행렬의 곱'을 형상에 주목해서 다시 확인한다. 행렬의 곱의 계산은 앞서 설명했지만, <mark>형상 확인</mark>이 굉장히 중요하다. $3 \\times 2$ 행렬 $\\textbf{A}$와 $2 \\times 4$ 행렬 $\\textbf{B}$을 곱하여 $3\\times4$ 행렬  $\\textbf{C}$을 만드는 예이다. 이때, 행렬 $\\textbf{A}$와 $\\textbf{b}$가 대응하는 차원의 원소 수가 같아야 한다. 행렬의 '형상 확인'이다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>2. 신경망의 추론</h2>\n",
    "<p> \n",
    "    신경망에서 수행하는 작업은 두 단계로 나눌 수 있다. '학습'과 '추론'이다. \n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>2-1 신경망 추론 전체 그림</h3>\n",
    "<p> \n",
    "    신경망은 간단히 말하면 '함수'라고 할 수 있다. 함수는 무엇인가 입력하면 무엇인가 출력하는 변환기다. 즉, 신경망도 입력을 출력으로 바꾸어 준다.\n",
    "    신경망은 <mark>입력층</mark>과, <mark>출력층</mark>, <mark>은닉층</mark>으로 이루어져 있었고 적당한 수의 뉴런들을 각 층(layer)에 배치한다. 뉴런을 $\\circ$로, 그 사이 연결을 화살표로 나타낸다. 화살표에는 <mark>가중치(weight)</mark>가 존재한다. 가중치와 뉴런의 값을 각각 곱해서 그 합이 다음 뉴런의 입력으로 쓰인다. 이때 각 층에는 이전 뉴런의 값에 영향받지 않는 '정수'도 더해진다. 이 정수는 <mark>편향(bias)</mark> 이라고 한다. 인접하는 층의 모든 뉴런과 연결되어 있다면 완전연결계층(fully connected layer) 라고 한다.\n",
    "</p>\n",
    "<p>\n",
    "    \n",
    "</p>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    신경망이 수행하는 계산을 수식으로 나타낸다. 입력층의 데이터를 $(x_1, x_2) $로 쓰고, 가중치 $w_11$과 $w_21$으로, 편향은 $b_1$으로 쓴다. 은닉층 중 첫번째 뉴런은 다음과 같이 계산할 수 있다.\n",
    "    $$\n",
    "        h_1 = x_1 w_{11} + x_2 w_{21} + b_1\n",
    "    $$\n",
    "    은닉층의 뉴런은 가중치의 합으로 계산된다. 뉴런의 수 만큼 반복하면 은닉층에 속한 모든 뉴런의 값을 구할 수 있다. 입력과 가중치는 가중치 합으로 계산되고 그 값은 행렬의 곱으로 한꺼번에 계산할 수 있다. 실제 완전 연결계층이 수행하는 변환은 행렬의 곱을 이용해 다음처럼 정리할 수 있다.\n",
    "    $$\n",
    "        (h_1, h_2, h_3, h_4) = (x_1, x_2)\\begin{pmatrix}{w_{11}}&{w_{12}}&{w_{13}}&{w_{14}}\\\\ {w_{21}}&{w_{22}}&{w_{23}}&{w_{24}} \\end{pmatrix} + (b_1, b_2, b_3, b_4)\n",
    "    $$\n",
    "    은닉층의 뉴런들은 (h1, h2, h3, h4)로 정리되며, $1 \\times 4$ 행렬로 간주할 수 있다. (혹은 행백터) 입력 (x1, x2)는 $1 \\times 2$ 행렬이고, 가중치는 $2 \\times 4$ 행렬, 편향은 $1\\times 4$ 행렬이다. 그리고 다음처럼 간소화 할 수 있다.\n",
    "    $$\n",
    "        \\textbf{h} = \\textbf{xW} + \\textbf{b}\n",
    "    $$ \n",
    "    행렬의 곱에서는 대응하는 차원의 원소 수가 같아야 한다. 행렬의 형상을 보면 올바른 변환인지 확인가능하다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    지금까지 수행한 변환은 하나의 샘플 데이터(입력 데이터)만을 대상으로 했다. 하지만 신경망의 추론이나 학습은 다수의 샘플 데이터(미니배치)를 한꺼번에 처리한다. N개의 샘플 데이터를 미니배치로 한꺼번에 처리할 수 있다. 이때 N개의 샘플 데이터가 한꺼번에 변환되고, 은닉측에는 N개 분의 뉴런이 함께 계산된다. \n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "\n",
    "W1 = np.random.randn(2, 4)  # 가중치\n",
    "b1 = np.random.randn(4)     # 편향\n",
    "x = np.random.randn(10, 2)  # 입력\n",
    "\n",
    "h = np.matmul(x, W1) + b1   # b1 브로드캐스트 실행\n",
    "print(h)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.04154897 -1.17705446 -1.3581127   1.63566493]\n",
      " [ 0.19272097 -0.76050405 -0.48872862  1.53993705]\n",
      " [-0.48985759 -2.32241608 -3.68722744  1.79709594]\n",
      " [-0.78357284 -1.91470855 -2.60208508  1.31496918]\n",
      " [-0.8358649  -1.85617752 -2.440931    1.23684925]\n",
      " [ 0.45913919 -0.38555823  0.22470258  1.56840158]\n",
      " [-0.5471589  -1.11042604 -0.89404813  1.08135212]\n",
      " [ 0.99674266  0.26438132  1.42119122  1.68439404]\n",
      " [-1.46273917 -1.84564421 -2.08449914  0.67976597]\n",
      " [-1.75765276 -2.3581496  -3.09639381  0.70175793]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    완전연결계층에 의한 변환은 '선형'변환이다. '비선형' 효과를 부여하는 것이 바로 <mark>활성화 함수</mark>이다. 비선형 활성화 함수를 이용해서 신경망의 표현력을 높일 수 있다. 활성화 함수는 다양하지만, 요번에 시그모이드 함수를 사용한다.\n",
    "    $$ \n",
    "        \\sigma(x) = \\frac{1}{1+exp(-x)}\n",
    "    $$\n",
    "    시그모이드 함수는 임의의 실수를 입력받아 0에서 1사이의 실수를 출력한다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "a = sigmoid(h)\n",
    "print(a)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.51038575 0.23558223 0.20454721 0.8369442 ]\n",
      " [0.54803167 0.31853684 0.38019312 0.82345557]\n",
      " [0.37992712 0.08928341 0.02442959 0.85779506]\n",
      " [0.31355037 0.12845279 0.06900435 0.78834349]\n",
      " [0.3024064  0.13514922 0.08010428 0.7750151 ]\n",
      " [0.61280995 0.40478702 0.55594047 0.82755562]\n",
      " [0.36652382 0.24779147 0.29027514 0.74674978]\n",
      " [0.73041766 0.56571301 0.80552509 0.8434855 ]\n",
      " [0.18804873 0.13638513 0.11061258 0.66368646]\n",
      " [0.14708456 0.08642018 0.04325625 0.66857741]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    시그모이드 함수에 의해 비선형 변환이 가능해진다. 계속해서 이 활성화 함수의 출력 a를 다른 완전연결계층에 통과시켜 변환한다. 지금 예에서는 은닉층의 뉴런 4개, 출력층의 뉴런은 3개이므로 완전연결계층에 사용되는 가중치 행렬은 $4 \\times 3 $ 형상으로 설정해야 된다. \n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.random.randn(10, 2)\n",
    "W1 = np.random.randn(2, 4)\n",
    "b1 = np.random.randn(4)\n",
    "W2 = np.random.randn(4, 3)\n",
    "b2 = np.random.randn(3)\n",
    "\n",
    "h = np.matmul(x, W1) + b1\n",
    "a = sigmoid(h)\n",
    "s = np.matmul(a, W2) + b2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    10개의 입력 데이터를 이용하여 3차원 데이터로 변형된다. 각 차원의 값을 이용하여 3클래스 분류를 할 수 있다. 출력된 3차원 벡터의 각 차원(원소)은 각 클래스에 대응하는 '점수(score)'가 된다. 분류를 한다면 점수가 큰 뉴런에 해당하는 클래스가 예측 결과가 된다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> 2-2 계층으로 클래스화 및 순전파 구현</h3>\n",
    "<p>\n",
    "    신경망에서 하는 처리를 계층(layer)로 구현해보자. 여기서는 완전연결계층에 의한 변환을 Affine 계층으로, 시그모이드 함수에 의한 변환을 Sigmoid 계층으로 구현한다. 기본 변환을 수행하는 메서드의 이름은 forward()이다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    계층들을 모두 파이썬 클래스로 구현한다. 이렇게 모듈화를 해두면 레고 블록을 조합하듯 신경망을 구축할 수 있다. 계층을 구현할 때 2가지 구현 규칙을 따른다.\n",
    "    <ul>\n",
    "        <li>모든 계층은 forward()와 backward() 메서드를 가진다.</li>\n",
    "        <li>모든 계층은 인스턴스 변수인 params와 grads를 가진다.</li>\n",
    "    </ul>\n",
    "    forward()와 backward() 메서드는 각각 순전파와 역전파를 수행한다. params는 가중치와 편향 같은 매개변수를 담는 리스트이다. grads는 params에 저장된 매개변수에 대응하여, 해당 매개변수에 대응하여, 매개변수의 <mark>기울기</mark>를 보관하는 리스트이다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self) -> None:\n",
    "        self.params = []    # 활성화 함수는 딱히 학습시키지 않는다. \n",
    "                            # 함수로 입력과 출력이 정확히 정리 되어있다.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# ch01/forward_net.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b) -> None:\n",
    "        self.params = [W, b]    # 초기화시 가중치와 편향을 받는다.\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.matmul(x, W) + b\n",
    "        return out\n",
    "\n",
    "# ch01/forward_net.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size) -> None:\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "\n",
    "        # 가중치와 편향 초기화\n",
    "        W1 = np.random.randn(I, H)\n",
    "        b1 = np.random.randn(H)\n",
    "        W2 = np.random.randn(H, O)\n",
    "        b2 = np.random.randn(O)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        \n",
    "        # 모든 가중치를 리스트에 모은다.\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    클래스의 초기화 메서드(__init__)는 먼저 가중치를 초기화하고 3개의 계층을 생성한다. 학습해야할 매개변수들을 params 리스트에 저장한다. 매개변수들을 하나의 리스트에 보관하면 '매개변수 갱신'과 '매개변수 저장'을 손쉽게 처리할 수 있다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "a = ['A', 'B']\n",
    "a += ['C', 'D']\n",
    "a\n",
    "\n",
    "# 리스트끼리 결합"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['A', 'B', 'C', 'D']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "s = model.predict(x)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cab6904625ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> 3. 신경망의 학습</h2>\n",
    "\n",
    "<p> \n",
    "    학습되지 않은 신경망으로는 훌륭한 추론은 못한다. 학습을 먼저 수행하고, 학습된 매개변수를 이용해 추론을 수행하는 흐름이 일반적이다. 추론은 다중 클래스 분류 등의 문제에 <mark>문제에 답을 구하는 작업</mark>이다. 신경망의 학습은 최적의 매개변수 값을 찾는 작업이다. \n",
    "</p>\n",
    "\n",
    "<h3> 손실함수 </h3>\n",
    "\n",
    "<p>\n",
    "    신경망 학습은 학습이 얼마나 잘 되고 있는지 알기 위한 '척도'가 필요하다. 일반적으로 학습 단계의 특정 시점에서 신경망의 성능을 나타내는 척도로 <mark>손실</mark>을 사용한다. 손실은 정답 데이터와 신경망이 예측한 결과를 비교하여 예측이 얼마나 <mark>나쁜가</mark>를 산출한 단일 값(스칼라) 이다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    신경망의 손실은 <mark>손실 함수</mark>  를 사용하여 구한다. 다중 클래스 분류 신경망은 손실 함수로 흔히 <b>교차 엔트로피 오차</b>를 이용한다. 교차 엔트로피 오차는 신경망이 출력하는 각 클래스의 '확률'과 '정답 레이블'을 이용해 구할 수 있다. $\\textbf{L}$은 손실을 나타낸다. Softmax 계층의 출력은 확률이 되어, Cross Entropy Error 계층에는 확률과 정답 레이블이 입력된다. \n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    소프트맥스 함수 식으로 쓰면 다음과 같다.\n",
    "    $$\n",
    "        y_k = \\frac{exp(s_k)}{\\sum_{i=1}^{n} exp(s_i)}\n",
    "    $$\n",
    "    출력이 총 n개일 때, k번째의 출력 $y_k$를 구하는 계산식이다. $y_k$는 k번째 클래스에 해당하는 소프트맥스 함수의 출력이다. 소프트맥스 함수의 분자는 점수 $s_k$의 지수 함수이고, 분모는 모든 입력 신호의 지수 함수의 총합이다.\n",
    "    소프트맥스 함수의 출력의 각 원소는 0.0 이상 1.0 이하의 실수이다. 그리고 그 원소들을 모두 더하면 1.0.이 된다. 그래서 소프트맥스의 출력을 '확률'로 해석할 수 있다. 소프트맥스 출력인 '확률'이 다음 차례인 교차 엔트로피 오차에 입력된다. 교차 엔트로피 오차의 수식은 다음과 같다\n",
    "    $$\n",
    "        L = - \\sum_{k} t_klogy_k\n",
    "    $$ \n",
    "    $t_k$는 k번째 클래스에 해당하는 정답 레이블이다. log는 네이피어 상수(혹은 오일러의 수) e를 밑으로 하는 로그이다. 정답 레이블은 t = [0, 0, 1]과 같이 원핫 벡터로 표기한다. 원핫 벡터는 단 하나의 원소만 1이고, 그 외는 0인 벡터이다. 1이 정답 클래스에 해당한다. \n",
    "</p>\n",
    "<p>\n",
    "    미니배치 처리도 고려하면 교차 엔트로피 오차의 식은 다음과 같다. 이 식에서 데이터는 N개이며, t_{nk}는 n번째 데이터의 k차원째의 값을 의미한다. 그리고 $y_{nk}$는 신경망의 출력이고, $t_{nk}$는 정답 레이블이다.\n",
    "    $$\n",
    "        L = - \\frac{1}{N}\\sum_{n}\\sum_{k}t_{nk}logy_{nk}\n",
    "    $$\n",
    "    미니배치에서 다른 점은 N으로 나눠서 1개당의 <mark>'평균 손실 함수'</mark>를 구한다. 이렇게 평균을 구함으로 미니배치의 크기에 관계없이 항상 일관된 척도를 얻을 수 있다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    소프맥스 함수와 교차 엔트로피 오차를 계산하는 계층을 Softmax with Loss 계층 하나로 구현한다. 그 구현은 따로 설명하지 않는다. common/layers.py에 있으니 확인 바란다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# common/layers.py\n",
    "# common/functions.py\n",
    "\"\"\"\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 정답 데이터가 원핫 벡터일 경우 정답 레이블 인덱스로 변환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None  # softmax의 출력 \n",
    "        self.t = None  # 정답 레이블\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        # 정답 레이블이 원핫 벡터일 경우 정답의 인덱스로 변환\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "\n",
    "        return dx\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nclass SoftmaxWithLoss:\\n    def __init__(self):\\n        self.params, self.grads = [], []\\n        self.y = None  # softmax의 출력 \\n        self.t = None  # 정답 레이블\\n\\n    def forward(self, x, t):\\n        self.t = t\\n        self.y = softmax(x)\\n\\n        # 정답 레이블이 원핫 벡터일 경우 정답의 인덱스로 변환\\n        if self.t.size == self.y.size:\\n            self.t = self.t.argmax(axis=1)\\n\\n        loss = cross_entropy_error(self.y, self.t)\\n        return loss\\n\\n    def backward(self, dout=1):\\n        batch_size = self.t.shape[0]\\n\\n        dx = self.y.copy()\\n        dx[np.arange(batch_size), self.t] -= 1\\n        dx *= dout\\n        dx = dx / batch_size\\n\\n        return dx\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>3-2 미분과 기울기</h3>\n",
    "\n",
    "<p>\n",
    "    신경망 학습의 목표는 <mark>손실을 최소화하는 매개변수를 찾는 것</mark>이다. 중요한 것은 '미분'과 '기울기'이다. 어떤 함수 $ y = f(x) $가 있다고 가정한다. 이때 x에 관한 y의 미분은 $\\frac{dy}{dx}$라고 쓴다. 이 $\\frac{dy}{dx}$ 가 의미하는  것은 x의 값을 '조금' 변화시켰을 때 y 값이 얼마나 변하는가 하는 '변화의 정도' 이다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    위에서는 x라는 변수 하나에 대해 구했지만, 여러 개의 변수(다변수)라도 마찬가지로 미분할 수 있다. 예를 들어 L은 스칼라, x는 벡터인 함수 L = f(x) 가 있다. 이때 $x_i$에 대한 L의 미분은 $\\frac{\\partial{L}}{\\partial{x_i}}$로 쓸 수 있다. \n",
    "    $$\n",
    "        \\frac{\\partial{L}}{\\partial{\\textbf{x}}} \n",
    "        = \\begin{pmatrix} {\\frac{\\partial{L}}{\\partial{x_1}}},{\\frac{\\partial{L}}{\\partial{x_2}}}, {\\cdots}, {\\frac{\\partial{L}}{\\partial{x_n}}}\\end{pmatrix}\n",
    "    $$\n",
    "    이 처럼 벡터의 각 원소에 대한 미분을 정리한 것이 기울기다. 행렬에서도 기울기를 생각할 수 있다. <b>W</b>가 $ m \\times n $ 행렬이라면, $ L = g(\\textbf{W}) $ 함수의 기울기는 다음과 같이 쓸 수 있다. \n",
    "    $$\n",
    "        \\frac{\\partial{L}}{\\partial{\\textbf{W}}} =\n",
    "        \\begin{pmatrix} \n",
    "        {\\frac{\\partial{L}}{\\partial{W_{11}}}} & {\\cdots} & {\\frac{\\partial{L}}{\\partial{W_{1n}}}} \\\\\n",
    "        {\\vdots} & {\\ddots} & {} \\\\\n",
    "        {\\frac{\\partial{L}}{\\partial{W_{m1}}}} & { } & {\\frac{\\partial{L}}{\\partial{W_{mn}}}} \n",
    "        \\end{pmatrix}\n",
    "    $$\n",
    "    L의 <b>W</b>에 대한 기울기를 행렬로 정리할 수 있다. 중요한 점은 <b>W</b>와 $\\frac{\\partial{L}}{\\partial{\\textbf{W}}}$의 형상이 같다. 위 성질을 이용하면 매개변수 갱신과 연쇄 법칙을 쉽게 구현할 수 있다. \n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>3-3 연쇄 법칙</h3>\n",
    "\n",
    "<p> \n",
    "    학습 시 신경망은 학습 데이터를 주면 손실을 출력한다. 손실에서 우리가 얻고 싶은 것은 매개변수에 대한 손실의 기울기이다. 기울기를 얻을 수 있으면 그것을 사용해 매개변수를 갱신할 수 있기 때문이다. 신경망의 기울기는 <b>오차역전파법</b>을 이용하여 구할 수 있다. <span style=\"text-decoration: line-through\"> 오차역전파법... </span> <br />\n",
    "    오차역전파법에서 <b>연쇄 법칙</b>이 중요하다. 연쇄 법칙이란 <mark>합성함수에 대한 미분의 법칙</mark>이다. \n",
    "</p>\n",
    "<p>\n",
    "    여기에 $ y = f(x)$ 와 $ z = g(y) $ 라는 두 함수가 있다. 그러면 $ z = g(f(x)))$가 되어, 출력 z는 두 함수를 조합해 계산할 수 있다. 이때 이 합성함수의 미분(x에대한 z의 미분)은 다음과 같이 구할 수 있다.\n",
    "    $$\n",
    "        \\frac{\\partial{z}}{\\partial{x}} = \\frac{\\partial{z}}{\\partial{y}} \\frac{\\partial{y}}{\\partial{x}}\n",
    "    $$\n",
    "    x에 대한 z의 미분은 y = f(x) 미분과 z = g(y) 미분을 곱하면 구해진다. 이 연쇄 법칙이 중요한 이유는 우리가 다루는 함수가 아무리 복잡하고 많은 함수와 연결되어 있더라도, 그 미문은 개별 함수의 미분들을 이용해 구할 수 있기 때문이다. 각 함수의 국소적인 미분을 계산할 수 있다면 그 값들을 곱해서 전체의 미분을 구할 수 있다. \n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> 3-4 계산 그래프 </h3>\n",
    "\n",
    "<p>\n",
    "    계산 그래프는 계산 과정을 시각적으로 보여준다. 계산 그래프는 노드와 화살표로 그린다. \n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    <b>덧셈 노드</b> <br />\n",
    "        순전파 <br />\n",
    "        $z = x + y$ <br />\n",
    "        역전파 <br />\n",
    "        $ \\frac{\\partial{L}}{\\partial{x}}\\quad = \\frac{\\partial{L}}{\\partial{z}} \\frac{\\partial{z}}{\\partial{x}} \\\\\n",
    "        \\quad\\quad = \\frac{\\partial{L}}{\\partial{z}} \\cdot 1 \\\\\n",
    "        \\frac{\\partial{L}}{\\partial{y}}\\quad = \\frac{\\partial{L}}{\\partial{z}} \\frac{\\partial{z}}{\\partial{y}} \\\\\n",
    "        \\quad\\quad = \\frac{\\partial{L}}{\\partial{z}} \\cdot 1 $\n",
    "\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    <b>곱셈 노드</b> <br />\n",
    "        순전파 <br />\n",
    "        $ z = x \\times y $ <br />\n",
    "        역전파 <br />\n",
    "        $ \n",
    "            \\frac{\\partial{L}}{\\partial{x}}\\quad = \\frac{\\partial{L}}{\\partial{z}} \\frac{\\partial{z}}{\\partial{x}} \\\\\n",
    "            \\quad\\quad = \\frac{\\partial{L}}{\\partial{z}} \\cdot y \\\\\n",
    "            \\frac{\\partial{L}}{\\partial{y}}\\quad = \\frac{\\partial{L}}{\\partial{z}} \\frac{\\partial{z}}{\\partial{y}} \\\\\n",
    "            \\quad\\quad = \\frac{\\partial{L}}{\\partial{z}} \\cdot x\n",
    "        $\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    <b>분기 노드</b> <br />\n",
    "        순전파 <br />\n",
    "        $ x = x $ <br />\n",
    "        역전파 <br />\n",
    "        $ \n",
    "            \\frac{\\partial{L}}{\\partial{x}} + \\frac{\\partial{L}}{\\partial{x}}\n",
    "        $\n",
    "</p>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    <b>Repeat 노드</b> <br />\n",
    "    분기 노드를 일반화하면 N개로의 분기 (복제)가 된다. \n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "\n",
    "D, N = 8, 7\n",
    "x = np.random.randn(1, D)               # 입력\n",
    "y = np.repeat(x, N, axis=0)             # 순전파\n",
    "dy = np.random.randn(N, D)              # 무작위 기울기\n",
    "dx = np.sum(dy, axis=0, keepdims=True)  # 역전파"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    <b>Sum 노드</b> <br />\n",
    "    Sum 노드는 범용 덧셈 노드이다. \n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "D, N = 8, 7\r\n",
    "x = np.random.randn(N, D)\r\n",
    "y = np.sum(x, axis=0, keepdims=True)\r\n",
    "\r\n",
    "dy = np.random.randn(1, D)\r\n",
    "dx = np.repeat(dy, N, axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    <b>MatMul 노드</b> <br />\n",
    "    MatMul 노드는 행렬의 곱셈을 MatMul 노드로 표현된다. $ \\textbf{y} = \\textbf{xW} $라는 계산으로 역전파를 계산한다. $ \\textbf{x, W, y} $ 의 형상은 각각 $ 1\\times D, D \\times H, 1 \\times H $ 이다.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    $$\n",
    "        x = (x_1, x_2, \\cdots, x_D) \\\\\n",
    "        y = (y_1, y_2, \\cdots, y_H) \\\\\n",
    "        W = \\begin{pmatrix} \n",
    "                {W_{11}}&{\\cdots}&{W_{1H}} \\\\\n",
    "                {\\vdots}&{\\ddots}&{} \\\\\n",
    "                {W_{D1}}&{}&{W_{DH}} \n",
    "            \\end{pmatrix} \\\\\n",
    "        \\frac{\\partial{L}}{\\partial{x_i}} = \\sum_{j}\\frac{\\partial{L}}{\\partial{y_j}} \\frac{\\partial{y_j}}{\\partial{x_i}} \\\\\n",
    "        \\frac{\\partial{y_j}}{\\partial{x_i}} = W_{ij} \\\\\n",
    "        \\frac{\\partial{L}}{\\partial{x_i}} = \\sum_{j}\\frac{\\partial{L}} \n",
    "        {\\partial{y_j}} \\frac{\\partial{y_j}}{\\partial{x_i}} = \\sum_{j}\\frac{\\partial{L}}{\\partial{y_j}}W_{ij} \\\\\n",
    "        \\frac{\\partial{L}}{\\partial{\\textbf{x}}} = \\frac{\\partial{L}}{\\partial{\\textbf{y}}}W^T\n",
    "    $$\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# common/layers.py\r\n",
    "class MatMul:\r\n",
    "    def __init__(self, W):\r\n",
    "        self.params = [W]\r\n",
    "        self.grads = [np.zeros_like(W)]\r\n",
    "        self.x = None\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        W, = self.params\r\n",
    "        out = np.matmul(x, W)\r\n",
    "        self.x = x\r\n",
    "        return out\r\n",
    "\r\n",
    "    def backward(self, dout):\r\n",
    "        W, = self.params\r\n",
    "        dx = np.matmul(dout, W.T)\r\n",
    "        dW = np.matmul(self.x.T, dout)\r\n",
    "        self.grads[0][...] = dW\r\n",
    "        return dx"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "934bad200ce2233862c51537af2a279b3e2fd7629fe4796e40f0bd88eed44efe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}