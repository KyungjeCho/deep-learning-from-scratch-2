{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1>신경망 복습</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> 밑바닥부터 시작하는 딥러닝의 속편입니다. 이번 장에서 신경망을 복습합니다.</h3>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> 1. 수학과 파이썬 복습</h2>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "먼저 수학을 복습합니다. 신경망 계산을 위한 '벡터'와 '행렬'에 대해 복습합니다. <hr />"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>1-1 벡터와 행렬 </h3>\n",
    "<p> 신경망은 벡터와 행렬이 도처에 등장한다. </p>\n",
    "<p>\n",
    "    '벡터'는 <mark> 크기와 방향을 가진 양</mark> 이다. 숫자가 일렬로 늘어선 집합으로 표현하고 또 그렇게 쓴다. 파이썬은 벡터를 1차원 배열로 취급한다. 즉, 파이썬에서는 '벡터'는 숫자로 집합된 1차원 배열이라고 생각하면 된다. \n",
    "</p>\n",
    "<p>\n",
    "    '행렬'은 숫자가 <mark> 2차원 형태로 늘어선 것 </mark> 으로 벡터 두개를 붙여 놓은 것 같이 보인다. $ N \\times M $ 으로 표현한다. 또 행렬에서는 가로줄을 <mark>행</mark>, 세로줄을 <mark>열</mark>라고 한다.\n",
    "</p>\n",
    "<p>\n",
    "    $$ \\begin{pmatrix}1&2\\\\3&4\\\\5&6\\end{pmatrix} $$\n",
    "</p>\n",
    "<p>\n",
    "    벡터는 표현하는 방법이 두 가지입니다. 하나는 숫자를 <mark>세로로 나열하는 방법(열백터)</mark> 과 <mark> 다른 하나는 가로로 나열하는 방법(행백터)</mark> 입니다. 수학과 딥러닝 등 많은 분야에서 '열백터' 방식을 선호한다. 또한 수식에서 벡터나 행렬은 $\\textbf{x}$와 $\\textbf{W}$ 처럼 굵게 표기하여 단일 원소로 이뤄진 스칼라 값과 구별했습니다.\n",
    "</p>\n",
    "    "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "x = np.array([1, 2, 3])\r\n",
    "print(x.__class__) # x 의 클래스\r\n",
    "\r\n",
    "print(x.shape) # x의 벡터 차원 형상, x는 3차원 벡터\r\n",
    "# 결과 : (3, )\r\n",
    "\r\n",
    "print(x.ndim) # x의 숫자 집합 N 차원 수, x는 1차원 \r\n",
    "# 결과 : 1\r\n",
    "\r\n",
    "W = np.array([[1, 2, 3], [4, 5, 6]])\r\n",
    "print(W.shape) # W의 행렬 차원 형상, W는 2 X 3 차원\r\n",
    "# 결과 : (2, 3)\r\n",
    "\r\n",
    "print(W.ndim) # W의 수자 집합 N 차원 수, W는 2차원\r\n",
    "# 결과 : 2"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3,)\n",
      "1\n",
      "(2, 3)\n",
      "2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    벡터와 행렬은 np.array() 메서드로 생성 가능하다. 이 메서드는 넘파이의 다차원 배열 클래스인 np.ndarray 클래스를 생성한다. np.ndarray 클래스에는 다양한 편의 메서드와 인스턴스 변수가 준비되어 있다. 그 중 shape와 ndim을 이용했다. shape는 <mark>다차원 배열의 형상</mark>을, ndim은 <mark>차원 수</mark>를 담고 있다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> 1-2 행렬의 원소별 연산</h3>\n",
    "<p> 이번에는 벡터와 행렬을 사용해 간단한 계산을 한다</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "W = np.array([[1, 2, 3], [4, 5, 6]])\r\n",
    "X = np.array([[0, 1, 2], [3, 4, 5]]) # 두 개의 같은 형상(shape) 행렬을 생성했다.\r\n",
    "\r\n",
    "print(W + X) # 행렬 덧셈 계산\r\n",
    "\r\n",
    "print(W * X)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 1  3  5]\n",
      " [ 7  9 11]]\n",
      "[[ 0  2  6]\n",
      " [12 20 30]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    다차원 넘파이 배열의 사칙연산 중 더하기($+$)와 곱하기($\\times$)를 했다. 피연산자인 다차원 배열들 서로 같은 행과 열에 있는 원소끼리 연산이 이루어 진다. \n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> 1-3 브로드 캐스트</h3>\n",
    "<p> 넘파이에서 다차원 배열과 형상(shape)이 다른 배열끼리도 연산이 가능하다. 예를 들면, $ 2 \\times 2 $ 행렬과 스칼라랑 연산할 수 있다.</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "A = np.array([[1, 2], [3, 4]]) # 2 * 2 행렬 \r\n",
    "print(A * 10) # 2 * 2 행렬과 스칼라(10)이랑 곱셈 연산"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[10 20]\n",
      " [30 40]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    $ 2 \\times 2 $ 행렬 A에 10인 스칼라를 곱했다. 이러면 스칼라 값 10이 $ 2 \\times 2 $ 행렬로 확장, 늘려진 행렬로 만들어진 후 원소별 연산을 수행한다. 이 기능을 <mark>브로드캐스트(broadcast)</mark>라고 한다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "A = np.array([[1, 2], [3, 4]]) \r\n",
    "b = np.array([[10, 20]]) \r\n",
    "# 2 * 2 행렬과 벡터를 선언했다.\r\n",
    "\r\n",
    "print(A * b) # 행렬과 백터를 원소별 연산을 시켰다."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[10 40]\n",
      " [30 80]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    여기서는 b가 행렬 A에 맞쳐서 $ 2 \\times 2 $ 크기로 확장되어\n",
    "    $$\n",
    "        b =  \\begin{pmatrix}10&20\\\\10&20\\end{pmatrix}\n",
    "    $$\n",
    "    로 되었다. 물론 브로드캐스트가 효과적으로 작동하려면 몇가지 규칙을 충족해야 된다.\n",
    "\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>1-4 벡터의 내적과 행렬의 곱 </h3>\n",
    "<p> \n",
    "    벡터의 내적과 행렬의 곱셈을 한다. 우선 백터의 내적부터 시작한다. 백터의 내적의 수식은 \n",
    "    $$\n",
    "        \\textbf{x} \\cdot \\textbf{y} = x_1 y_1 + x_2 y_2 + \\cdots + x_n y_n\n",
    "    $$\n",
    "    이다. 벡터의 내적은 두 벡터에서 대응하는 원소들의 곱을 모두 더한 것이다. 이러면 결과는 스칼라가 된다. 백터의 내적은 직관적으로 <mark>두 벡터가 얼마나 같은 방향을 향하고 있는지</mark>  보여준다. 완전히 같은 방향이면 내적의 결과는 1이고, 반대 방향을 향하면 두 벡터의 내적은 -1이다.\n",
    "</p>\n",
    "<p>\n",
    "    행렬의 곱도 한다. 행렬의 곱은 '왼쪽 행렬의 행벡터(가로방향)'와 '오른쪽 행렬의 열벡터(세로방향)'의 내적으로 계산한다. 그리고 계산 결과는 새로운 행렬의 대응하는 원소에 저장한다. $\\textbf{A}$의 1행과 $\\textbf{B}$의 1열의 계산 결과는 1행1열 위치의 원소로 된다. $ 2 \\times 3 $ 행렬과 $ 3 \\times 2 $ 행렬을 곱하면 결과는 $ 2 \\times 2 $ 행렬이 된다. 그리고 왼쪽 행렬의 행벡터 차원과 오른쪽 행렬의 열벡터 차원의 수가 같아야 한다.\n",
    "</p>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# 벡터의 내적\r\n",
    "a = np.array([1, 2, 3])\r\n",
    "b = np.array([4, 5, 6])\r\n",
    "print(np.dot(a, b)) # 벡터의 내적은 np.dot() 메서드를 이용한다. 두 벡터의 형상(shape)은 같다.\r\n",
    "\r\n",
    "# 행렬의 곱\r\n",
    "A = np.array([[1, 2], [3, 4]])\r\n",
    "B = np.array([[5, 6], [7, 8]])\r\n",
    "print(np.matmul(A, B)) # 행렬의 곱은 np.matmul() 메서드를 이용한다."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32\n",
      "[[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\n",
    "    사실 벡터의 내적과 행렬의 곱 모두 np.dot() 메서드를 사용할 수 있다. np.dot(x, y)의 인수가 모두 1차원이면 벡터의 내적을 계산하고, 2차원 배열이면 행렬의 곱을 계산한다. 다만, 의도를 확실하게 해주어야 한다. 넘파이 경험을 쌓고 싶으면 '100 numpy exercise' 사이트를 추천한다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> 1-5 행렬 형상 확인 </h3>\n",
    "<p> \n",
    "    행렬이나 벡터를 사용할 때는 각각의 형상(shape)에 주의해야 한다. '행렬의 곱'을 형상에 주목해서 다시 확인한다. 행렬의 곱의 계산은 앞서 설명했지만, <mark>형상 확인</mark>이 굉장히 중요하다. $3 \\times 2$ 행렬 $\\textbf{A}$와 $2 \\times 4$ 행렬 $\\textbf{B}$을 곱하여 $3\\times4$ 행렬  $\\textbf{C}$을 만드는 예이다. 이때, 행렬 $\\textbf{A}$와 $\\textbf{b}$가 대응하는 차원의 원소 수가 같아야 한다. 행렬의 '형상 확인'이다.\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>2. 신경망의 추론</h2>\n",
    "<p> \n",
    "    신경망에서 수행하는 작업은 두 단계로 나눌 수 있다. '학습'과 '추론'이다. \n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>2-1 신경망 추론 전체 그림</h3>\n",
    "<p> \n",
    "    신경망은 간단히 말하면 '함수'라고 할 수 있다. 함수는 무엇인가 입력하면 무엇인가 출력하는 변환기다. 즉, 신경망도 입력을 출력으로 바꾸어 준다.\n",
    "    신경망은 <mark>입력층</mark>과, <mark>출력층</mark>, <mark>은닉층</mark>으로 이루어져 있었고 적당한 수의 뉴런들을 각 층(layer)에 배치한다. 뉴런을 $\\circ$로, 그 사이 연결을 화살표로 나타낸다. 화살표에는 <mark>가중치(weight)</mark>가 존재한다. 가중치와 뉴런의 값을 각각 곱해서 그 합이 다음 뉴런의 입력으로 쓰인다. 이때 각 층에는 이전 뉴런의 값에 영향받지 않는 '정수'도 더해진다. 이 정수는 <mark>편향(bias)</mark> 이라고 한다. 인접하는 층의 모든 뉴런과 연결되어 있다면 완전연결계층(fully connected layer) 라고 한다.\n",
    "</p>\n",
    "<p>\n",
    "    \n",
    "</p>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\r\n",
    "    신경망이 수행하는 계산을 수식으로 나타낸다. 입력층의 데이터를 $(x_1, x_2) $로 쓰고, 가중치 $w_11$과 $w_21$으로, 편향은 $b_1$으로 쓴다. 은닉층 중 첫번째 뉴런은 다음과 같이 계산할 수 있다.\r\n",
    "    $$\r\n",
    "        h_1 = x_1 w_{11} + x_2 w_{21} + b_1\r\n",
    "    $$\r\n",
    "    은닉층의 뉴런은 가중치의 합으로 계산된다. 뉴런의 수 만큼 반복하면 은닉층에 속한 모든 뉴런의 값을 구할 수 있다. 입력과 가중치는 가중치 합으로 계산되고 그 값은 행렬의 곱으로 한꺼번에 계산할 수 있다. 실제 완전 연결계층이 수행하는 변환은 행렬의 곱을 이용해 다음처럼 정리할 수 있다.\r\n",
    "    $$\r\n",
    "        (h_1, h_2, h_3, h_4) = (x_1, x_2)\\begin{pmatrix}{w_{11}}&{w_{12}}&{w_{13}}&{w_{14}}\\\\ {w_{21}}&{w_{22}}&{w_{23}}&{w_{24}} \\end{pmatrix} + (b_1, b_2, b_3, b_4)\r\n",
    "    $$\r\n",
    "    은닉층의 뉴런들은 (h1, h2, h3, h4)로 정리되며, $1 \\times 4$ 행렬로 간주할 수 있다. (혹은 행백터) 입력 (x1, x2)는 $1 \\times 2$ 행렬이고, 가중치는 $2 \\times 4$ 행렬, 편향은 $1\\times 4$ 행렬이다. 그리고 다음처럼 간소화 할 수 있다.\r\n",
    "    $$\r\n",
    "        \\textbf{h} = \\textbf{xW} + \\textbf{b}\r\n",
    "    $$ \r\n",
    "    행렬의 곱에서는 대응하는 차원의 원소 수가 같아야 한다. 행렬의 형상을 보면 올바른 변환인지 확인가능하다.\r\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\r\n",
    "    지금까지 수행한 변환은 하나의 샘플 데이터(입력 데이터)만을 대상으로 했다. 하지만 신경망의 추론이나 학습은 다수의 샘플 데이터(미니배치)를 한꺼번에 처리한다. N개의 샘플 데이터를 미니배치로 한꺼번에 처리할 수 있다. 이때 N개의 샘플 데이터가 한꺼번에 변환되고, 은닉측에는 N개 분의 뉴런이 함께 계산된다. \r\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "W1 = np.random.randn(2, 4)  # 가중치\r\n",
    "b1 = np.random.randn(4)     # 편향\r\n",
    "x = np.random.randn(10, 2)  # 입력\r\n",
    "\r\n",
    "h = np.matmul(x, W1) + b1   # b1 브로드캐스트 실행\r\n",
    "print(h)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.04154897 -1.17705446 -1.3581127   1.63566493]\n",
      " [ 0.19272097 -0.76050405 -0.48872862  1.53993705]\n",
      " [-0.48985759 -2.32241608 -3.68722744  1.79709594]\n",
      " [-0.78357284 -1.91470855 -2.60208508  1.31496918]\n",
      " [-0.8358649  -1.85617752 -2.440931    1.23684925]\n",
      " [ 0.45913919 -0.38555823  0.22470258  1.56840158]\n",
      " [-0.5471589  -1.11042604 -0.89404813  1.08135212]\n",
      " [ 0.99674266  0.26438132  1.42119122  1.68439404]\n",
      " [-1.46273917 -1.84564421 -2.08449914  0.67976597]\n",
      " [-1.75765276 -2.3581496  -3.09639381  0.70175793]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\r\n",
    "    완전연결계층에 의한 변환은 '선형'변환이다. '비선형' 효과를 부여하는 것이 바로 <mark>활성화 함수</mark>이다. 비선형 활성화 함수를 이용해서 신경망의 표현력을 높일 수 있다. 활성화 함수는 다양하지만, 요번에 시그모이드 함수를 사용한다.\r\n",
    "    $$ \r\n",
    "        \\sigma(x) = \\frac{1}{1+exp(-x)}\r\n",
    "    $$\r\n",
    "    시그모이드 함수는 임의의 실수를 입력받아 0에서 1사이의 실수를 출력한다.\r\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def sigmoid(x):\r\n",
    "    return 1 / (1 + np.exp(-x))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "a = sigmoid(h)\r\n",
    "print(a)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.51038575 0.23558223 0.20454721 0.8369442 ]\n",
      " [0.54803167 0.31853684 0.38019312 0.82345557]\n",
      " [0.37992712 0.08928341 0.02442959 0.85779506]\n",
      " [0.31355037 0.12845279 0.06900435 0.78834349]\n",
      " [0.3024064  0.13514922 0.08010428 0.7750151 ]\n",
      " [0.61280995 0.40478702 0.55594047 0.82755562]\n",
      " [0.36652382 0.24779147 0.29027514 0.74674978]\n",
      " [0.73041766 0.56571301 0.80552509 0.8434855 ]\n",
      " [0.18804873 0.13638513 0.11061258 0.66368646]\n",
      " [0.14708456 0.08642018 0.04325625 0.66857741]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\r\n",
    "    시그모이드 함수에 의해 비선형 변환이 가능해진다. 계속해서 이 활성화 함수의 출력 a를 다른 완전연결계층에 통과시켜 변환한다. 지금 예에서는 은닉층의 뉴런 4개, 출력층의 뉴런은 3개이므로 완전연결계층에 사용되는 가중치 행렬은 $4 \\times 3 $ 형상으로 설정해야 된다. \r\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "def sigmoid(x):\r\n",
    "    return 1 / (1 + np.exp(-x))\r\n",
    "\r\n",
    "x = np.random.randn(10, 2)\r\n",
    "W1 = np.random.randn(2, 4)\r\n",
    "b1 = np.random.randn(4)\r\n",
    "W2 = np.random.randn(4, 3)\r\n",
    "b2 = np.random.randn(3)\r\n",
    "\r\n",
    "h = np.matmul(x, W1) + b1\r\n",
    "a = sigmoid(h)\r\n",
    "s = np.matmul(a, W2) + b2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\r\n",
    "    10개의 입력 데이터를 이용하여 3차원 데이터로 변형된다. 각 차원의 값을 이용하여 3클래스 분류를 할 수 있다. 출력된 3차원 벡터의 각 차원(원소)은 각 클래스에 대응하는 '점수(score)'가 된다. 분류를 한다면 점수가 큰 뉴런에 해당하는 클래스가 예측 결과가 된다.\r\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> 2-2 계층으로 클래스화 및 순전파 구현</h3>\r\n",
    "<p>\r\n",
    "    신경망에서 하는 처리를 계층(layer)로 구현해보자. 여기서는 완전연결계층에 의한 변환을 Affine 계층으로, 시그모이드 함수에 의한 변환을 Sigmoid 계층으로 구현한다. 기본 변환을 수행하는 메서드의 이름은 forward()이다.\r\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\r\n",
    "    계층들을 모두 파이썬 클래스로 구현한다. 이렇게 모듈화를 해두면 레고 블록을 조합하듯 신경망을 구축할 수 있다. 계층을 구현할 때 2가지 구현 규칙을 따른다.\r\n",
    "    <ul>\r\n",
    "        <li>모든 계층은 forward()와 backward() 메서드를 가진다.</li>\r\n",
    "        <li>모든 계층은 인스턴스 변수인 params와 grads를 가진다.</li>\r\n",
    "    </ul>\r\n",
    "    forward()와 backward() 메서드는 각각 순전파와 역전파를 수행한다. params는 가중치와 편향 같은 매개변수를 담는 리스트이다. grads는 params에 저장된 매개변수에 대응하여, 해당 매개변수에 대응하여, 매개변수의 <mark>기울기</mark>를 보관하는 리스트이다.\r\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "class Sigmoid:\r\n",
    "    def __init__(self) -> None:\r\n",
    "        self.params = []    # 활성화 함수는 딱히 학습시키지 않는다. \r\n",
    "                            # 함수로 입력과 출력이 정확히 정리 되어있다.\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        return 1 / (1 + np.exp(-x))\r\n",
    "\r\n",
    "# ch01/forward_net.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "class Affine:\r\n",
    "    def __init__(self, W, b) -> None:\r\n",
    "        self.params = [W, b]    # 초기화시 가중치와 편향을 받는다.\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        W, b = self.params\r\n",
    "        out = np.matmul(x, W) + b\r\n",
    "        return out\r\n",
    "\r\n",
    "# ch01/forward_net.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "class TwoLayerNet:\r\n",
    "    def __init__(self, input_size, hidden_size, output_size) -> None:\r\n",
    "        I, H, O = input_size, hidden_size, output_size\r\n",
    "\r\n",
    "        # 가중치와 편향 초기화\r\n",
    "        W1 = np.random.randn(I, H)\r\n",
    "        b1 = np.random.randn(H)\r\n",
    "        W2 = np.random.randn(H, O)\r\n",
    "        b2 = np.random.randn(O)\r\n",
    "\r\n",
    "        # 계층 생성\r\n",
    "        self.layers = [\r\n",
    "            Affine(W1, b1),\r\n",
    "            Sigmoid(),\r\n",
    "            Affine(W2, b2)\r\n",
    "        ]\r\n",
    "        \r\n",
    "        # 모든 가중치를 리스트에 모은다.\r\n",
    "        self.params = []\r\n",
    "        for layer in self.layers:\r\n",
    "            self.params += layer.params\r\n",
    "    \r\n",
    "    def predict(self, x):\r\n",
    "        for layer in self.layers:\r\n",
    "            x = layer.forward(x)\r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\r\n",
    "    클래스의 초기화 메서드(__init__)는 먼저 가중치를 초기화하고 3개의 계층을 생성한다. 학습해야할 매개변수들을 params 리스트에 저장한다. 매개변수들을 하나의 리스트에 보관하면 '매개변수 갱신'과 '매개변수 저장'을 손쉽게 처리할 수 있다.\r\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "a = ['A', 'B']\r\n",
    "a += ['C', 'D']\r\n",
    "a\r\n",
    "\r\n",
    "# 리스트끼리 결합"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['A', 'B', 'C', 'D']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "x = np.random.randn(10, 2)\r\n",
    "model = TwoLayerNet(2, 4, 3)\r\n",
    "s = model.predict(x)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "934bad200ce2233862c51537af2a279b3e2fd7629fe4796e40f0bd88eed44efe"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}